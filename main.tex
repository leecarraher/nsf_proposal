\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage[small]{caption}
\usepackage{subcaption}

\usepackage{fancyhdr}
\DeclareGraphicsExtensions{.pdf,.eps,.svg,.pgm, .png}
    % EXTREMELY COMMON LaTeX PACKAGES TO INCLUDE:
%    \usepackage{amsmath,amsthm, amsfonts,amssymb} % For AMS Beautification
   % \usepackage{setspace} % For Single & Double Spacing Commands
    \usepackage[linktocpage,bookmarksopen,bookmarksnumbered,%PDF navigation
		pdftitle={RP Hash Clustering},%   and URL hyperlinks
		pdfauthor={Department of Electronic and Computing Systems},%
		pdfsubject={NSF Proposal},%
		pdfkeywords={UC}]{hyperref}

%\usepackage{caption3} % load caption package kernel first
%\DeclareCaptionOption{parskip}[]{} % disable "parskip" caption option
%\usepackage[small]{caption}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algcompatible}
%\usepackage{algorithm2e}
%\usepackage{algorithmic}



  \newtheorem{Theorem}{Theorem}[section]
    \newtheorem{Proposition}[Theorem]{Proposition}
    \newtheorem{Lemma}[Theorem]{Lemma}
    \newtheorem{Corollary}[Theorem]{Corollary}

        \newtheorem{Definition}[Theorem]{Definition}

\usepackage[section]{placeins}
\title{Random Projection Hashing for Scalable Big Data Clustering}
\begin{document}
\begin{center}
\LARGE{\textbf{Random Projection Hashing for Scalable Big Data Clustering}}
\vspace*{0.4in}

  {\large A thesis submitted to the\\[0.20in]
    Division of Research and Advanced Studies\\
    of the University of Cincinnati\\[0.20in]
    in partial fulfillment of the\\
    requirements for the degree of\\[0.20in]
    {\bf Doctor of Philosophy}\\[0.20in]
    in the School of Electric and Computing Systems\\
    of the College of Engineering and Applied Sciences\\[0.20in]
    April 06, 2013\\[0.20in]
    by\\[0.20in]
    {\bf Lee A Carraher}\\
    BSCE, University of Cincinnati, 2008
    MSCS,University of Cincinnati, 2012 \\
    \vspace*{0.4in}
  {\large Dissertation Advisor and Committee Chair:  Dr. Philip Wilsey}}
\end{center}
\pagenumbering{gobble}

\newpage
%\newpage
%\thispagestyle{empty}
%\mbox{}

%\setcounter{page}{1}
%\pagenumbering{roman}
%\clearpage
\pagestyle{fancy}
\section{Project Summary} 
\subsection*{Overview: } 
This proposal presents a
distributed algorithm for secure clustering of high dimensional data.  A novel
algorithm, called Random Projection Hash or \emph{RPHash}, utilizes aspects
of locality sensitive hashing (LSH) and multi-probe random projection for
computational scalability and linear achievable gains from parallel speed up.
The two step approach is data agnostic, minimizes communication overhead,
and has a priori predictable computational time.  The system is deployable
on commercially available cloud resources running the Hadoop (\emph{MR}v2)
implementation of MapReduce.  The \emph{RPHash} solution will have a wide
applicability to a variety of standard clustering applications while this
project will focus on a subset of clustering problems in the biological
data analysis space.  \emph{RPHash} also combats de-anonymization attacks
inherently resulting from its algorithmic requirements thus addressing
requirements involving the handling and privacy protection of health care
data\cite{presidential} as well as the inherent privacy concerns of using
cloud based services.  Furthermore, \emph{RPHash} will allow researchers to
scale their clustering problems without the need for specialized equipment
or computing resources.  The proposed cloud processing solution will allow
researchers to arbitrarily scale their processing needs using virtually
limitless commercial processing resources.

\subsection*{Intellectual Merit: } A principle driving force in computational
progress results from material and architectural advances in microprocessor
design. Many of these advances have been stagnated for linear processing
however due to thermal dissipation and energy requirements, resulting in a
shift toward parallel multi-processing.  Though much has been done to adapt
current algorithms for this parallel processing landscape, many solutions
tend to be a' posteriori methods favoring empirical speedup over long
term scalability. In this proposal, we develop a method for an algorithm
central to data analysis, designed expressly for parallel multi-processing
systems. In addition, our algorithm addresses often overlooked communication
bottlenecks in parallel design, through use of side channel synchronization
based on mathematically generative groups and probabilistic approximation. A
favorable side effect of the probabilistic approximation results in a possible
solution to de-anonymization attacks\cite{deanon1}\cite{deanon2} on user data.


\subsection*{Broader Impacts:} Clustering has long been the standard method
used for the analysis of labeled and unlabeled data.  Clusterings' effects
intrinsically identify the latent models underlying the distributions
of objects in a dataset, often unattainable through standard statistical
methods.  Single pass, data intensive statistical methods are often the
primary workhorses for parallel database processing of business logic and
other domains, while clustering is often overlooked due to scalability
ocncerns and confusion caused by the wide variety of available distributed
algorithms \cite{clusters}.  A multitude of surveys \cite{clusters} have
been made available comparing different aspects of clustering algorithms,
such as accuracy, complexity and application domain.  Fields in health and
biology are benefited by data clustering scalability.  Bioinformatic problems 
such as Micro Array clustering, Protein-Protein interaction clustering, medical
resource decision making, medical image processing, and clustering of 
epidemiological events all serve to benefit from larger dataset sizes. 
Furthermore, the proposed method provides data anonymization through destructive manipulation of the data preventing de-anonymization attacks beyond standard 
best practices database security methods.
\newpage
\pagenumbering{arabic}
\setcounter{section}{0}
\pagestyle{plain}
%\begin{abstract}
%In this proposal, a distributed algorithm for secure clustering of high 
%dimensional data is presented. \emph{RPHash} is a novel algorithm for data 
%clustering, that utilizes aspects of locality sensitive hashing (LSH) and 
%multi-probe random projection for computational scalability and linear 
%achievable gains from parallel speed up.  The two step approach in\emph{RPHash}
% is data agnostic, minimizes communication overhead, and has \'a priori 
%predictable computational time.  Furthermore, the \emph{RPHash} system will be 
%deployable on commercially available cloud resources running the Hadoop 
%(\emph{MR}v2) implementation of MapReduce.  As an added benefit, \emph{RPHash} 
%provides resilience to de-anonymization attacks inherently at no additional run
% time cost.\end{abstract}
\section{Project Overview}
Data clustering is
an inherent problem in data analysis.	It is the principle work horse of many
types of pilot data analysis and various data retrieval methods.  Due to
its importance in machine learning, data clustering is studied extensively
in computing and mathematics\cite{anderberg1973,Samet,clusters}.  In addition 
to advancements in data clustering, 
Many challenges to algorithm design are presented by the changes in computing architectures, resulting from changing use cases, market trends, and underlying 
physical design limits. Multi-processor architectures
and distributed computing are one such area of change, that is rapidly 
overtaking sequential computing 
prompting a fundamental restructuring in algorithmic design\cite{waldo1997}.
Furthermore, optimal coordination of many independent processors is an open
problem with fundamental communication scalability issues\cite{Amdahl}.
Random Projection Hash(\emph{RPHash}) was expressly created for minimizing
parallel communication overhead while providing algorithmic scalability.
The shortcomings of various other parallel methods will be discussed in
this proposal with suggestions on how \emph{RPHash} can overcome them.
Many parallel clustering algorithms have been converted to function
efficiently on parallel and distributed systems, however they often have
potential issues regarding their asymptotic scalability\cite{Proclus},
dimensionality in which they are effective\cite{Clarans}, and robustness
to noise.
\emph{RPHASH} is a method proposed for large scale distributed data clustering
combining locality sensitive hash functions and multi-probe random projection
for distributed approximate data clustering.  Clustering is an inherent
problem in data analysis in which a set of vectors follow a suspected latent
model distribution\cite{latent1,Hofmann,clusterstats,Zass2005}.  Although
many algorithms have been proposed both parallel friendly and sequential,
many issues still are present when applied to very large, high dimensional,
distributed datasets\cite{Clarans,Proclus,mpicluster,distributeddata,parclus}.
The system designed herein will focus on data security, algorithmic and
communication scalability as well as the potential gains from parallel speedup.
As the system being designed is proposed as a real implementation, beyond a
theoretical algorithmic design, another goal is that it is compatible with
commercial, cloud-based, distributed computing resources(e.g.  Amazon EC2,
Google Cloud, and similar offerings from IBM and Intel) as well as private
distributed processing systems(Beowulf, Hadoop).  Initially the proposed system
will provide a straightforward interface running on the popular MapReduce
(\emph{MR}v2)\cite{mapreduce,hadoop} parallel processing system.  From this
point, extension to cloud based resources is straightforward, allowing for
arbitrary resource scalability.  A system is proposed that will have wide
applicability to a variety of standard clustering applications while focusing
on a subset of clustering problems in the biological data analysis space.
Although many opportunities for clustering in biology exist, the focus of
this proposal will be on noisy, high dimensional, genomic expression data
gathered from cell samples\cite{anders2010differential}.  Furthermore, due
to new requirements involving the handling of health care data, as well as
the inherent privacy concerns of using cloud based services, data security
is primary to \emph{RPHash}'s interprocess communication architecture.

\subsection{RPHash: A Secure, Scalable Clustering Algorithm} The
\emph{RPHash} algorithm uses various recent techniques in data mining
along with a new approach toward achieving algorithmic scalability.
A large body of research has been applied to this field and in the past
has focused on shared memory architectures with heavy data bandwidth
requirements\cite{Zaki2000,distributeddata,parclus}.  However, recent
architectures changes and market shifts, promoted by the commercial successes
of Big Data and cloud based shared commodity hardware, have driven down
system bandwidth and reliance on high performance network architectures, in
favor of low bandwidth, high data throughput systems\cite{mapreduce,hadoop}.
The current set of database and high performance clustering algorithms have
had to shift their requirements and take on new responsibilities for these
contemporary systems to maintain utility in the ``Big Data'' clustering space.
\emph{RPHash} is an algorithm designed from the ground up with these
priorities and responsibility as its principle strength.  \emph{RPHash}
achieves this by minimizing interprocess communication requirements,
via an exchange for some redundant computation and an acceptable level
of approximation.  As with many parallel computations, and often computing
in general, memory access and synchronization bottlenecks often dominate
the computational complexity of an algorithm.  To an even greater extent,
theoretical speedup gains achievable from addition computing resources are
negatively effected following Amdahl's Law for scaled speedup\cite{Amdahl}.
The reasons for this have been addressed since the beginning of parallel
computation and are easily evidenced by the polynomial bound of the edges in
a completely connected graph.  For this reason, redundancy of computation
on a per compute node level is suggested as an acceptable trade-off, for
both system complexity and scalability, and hopefully can achieving similar
results of other communication intensive clustering algorithms.

Another of the goals of the \emph{RPHash} clustering engine is ease of use.
As it is intended for use in fields beyond computer engineering, it must be
very easy to deploy.  One way of providing this, is to remove the technical
hurdle of building and maintaining large scale computing systems.  Instead,
\emph{RPHash} relies on commercially provided, cloud computing platforms
running fully configured MRv2 virtual system nodes.  Furthermore, as the
distributed processing architecture is a commercially provided system, the
companies commercial ambitions will promote ongoing access to state-of-the-art
computing resources.  Cloud computing provides a pay-per-use computing
resources that scale with the computational requirements without burdening
researchers or their IT providers.

As \emph{RPHash} is intended for cloud deployment, data security should be
a primary concern.  Privacy concerns over attacks on de-identification,
or de-anonymization\cite{deanon1,deanon2} of thought to be anonymous
data, such as  whole genome sequence data\cite{deident}, has prompted a
presidential commission on the subject\cite{presidential}.  The results of
which add new restrictions to the way researchers acquire and process data.
While attempting to mitigate communication restrictions, \emph{RPHash}
intrinsically provides some security in the data it chooses to exchange.
Previous attempts at securing data in distributed systems required additional
cryptographic steps\cite{Lindell2000}.	Namely, the randomly projected hash
IDs, and the aggregation of only the k-largest cardinality vectors sets.
%An important aspects of the \emph{RPHash} system is its focus on system
%independent data security for private clouds, connected via the internet.
 Non-distributed data clustering requires the
entire dataset to reside on the processing system, while distributed
methods often require communication of full data records between nodes.
In the subspace projection step of \emph{RPHash}, nearly-orthogonal random
projection is utilized as a destructive operation, providing vector anonymity
for the data being during interprocess communication steps.  Furthermore,
fully qualified centroid vectors are communicate however, by construction,
these vectors represent the aggregate of many data records.  In the case of
patient data, an individuals private information is unlikely to be recoverable.

\subsection{Proposal Objectives} The overall goal of the proposal is
to create a secure, and scalable cloud based clustering system of high
dimensional data such as genomic data.	The following research objectives
will be addressed specifically:
\begin{enumerate}
\item \textbf{Develop the sequential \emph{RPHash} Algorithm:}
The first step in developing a parallel system is to develope the initial
sequential version of the \emph{RPHash} algorithm.  The overall algorithm 
excluding the decoding and random projection, is fairly straight forward.  An 
existing implementation of the GPGPU parallel Leech Lattice decoder and 
random projection methods from \cite{carraher} will be incorporated.

\item \textbf{Develop A distributed version of the proposed \emph{RPHash} 
Algorithm:}
The current sequential algorithm for random projection hashing has shown both 
accuracy and scalability promise, prompting the next step to extend it to the 
parallel architecture per its original design.  The parallel extension is a 
straightforward application of the naively parallel projection and decoding, 
along with a the standard log reduction sum of MapReduce\cite{mapreduce} and 
other parallel gather functions\cite{mpispec}.

\item \textbf{ Deploy \emph{RPHash} on Map Reduce(\emph{MR}v2) Hadoop 
framework for parallel computation:} Building the parallel \emph{RPHash} for 
the standard open source map reduce implementation will allow scalability 
testing of both performance and accuracy on local and publicly available cloud 
resources.  \emph{RPHash}'s expandability will be provided through a virtual 
machine core based system.  The virtual machine cores will allow \emph{RPHash} 
to be deployed on most cloud services that provide virtual machine support.


\item \textbf{Compare the Accuracy, Speedup, and Scalability of 
\emph{RPHash}:} 
The availability of state of the art data clustering algorithms for Hadoop, 
provided by the Mahout Data Mining library, will allow comparing the accuracy, 
performance and scalability of \emph{RPHash} against a variety of optimized 
alternatives.  Comparisons will be made based on overall clustering accuracy, 
such as precision recall, scaled speed, and overall problem size scalability.

\item \textbf{Show \emph{RPHash}'s innate resistance to de-anonymization 
attacks:} 
Although proving security without rigorous proof is a difficult problem, This 
proposal will empirically assert the resistance of \emph{RPHash} to various 
de-anonymization attacks and data scraping.  This rudimentary analysis, will 
focus on understanding the amount of destructiveness the random projection 
method has on data and its resistance to least squares inversions.
\end{enumerate}


\subsection{Impact}

In this proposal an algorithm for scalable data clustering on distributed
systems is developed.  The algorithm developed combines approximate and
randomized methods in a new way to solve issues of scalability and data
security for real world distributed data analysis.  The focus on a randomized,
and thus non-deterministic, clustering algorithm is somewhat uncommon in
computing, but common for ill-posed, combinatorially restrictive problems
like clustering and partitioning.  This assertion is complemented by a
similar inversion regarding clustering and computing, in which real world
problems tend to converge much faster than adversarial crafted worse-case
problems\cite{Mahajan09}.  The principle assumption in this proposal is that
approximate and exact clustering, are qualitative similar due to to noise,
redundancy, data loss, and the curse of dimensionality.  Furthermore, the
addition of random noise to the clustering dataset resulting from the random
subspace projection requirement, provides some degree of non-deterministic
process, so subsequent iterations of the algorithm could potentially find
better results.  Making the process of finding better clustering results,
a matter of available processing time and resources.

Clustering has long been the standard method used for the analysis of
labeled and unlabeled data.  Clusterings effects intrinsically identify
dissimilar and similar objects in a dataset, often unattainable through
standard statistical methods.  Single pass, data intensive statistical
methods are often the primary workhorses for parallel database processing
of business logic and other domains, while clustering is often overlooked
due to scalability and confusion caused by the wide variety of available
algorithms\cite{clusters}.  A multitude of surveys\cite{clusters} have
been made available comparing different aspects of clustering algorithms,
such as accuracy, complexity and application domain.  Fields in health and
biology are benefited by data clustering scalability.  Such fields as Micro
Array clustering, Protein-Protein interaction clustering, medical resource
decision making, medical image processing,  and clustering of epidemiological
events all serve to benefit from larger dataset sizes.

An objective of this proposal is to create a highly scalable, cloud based,
data clustering system for health care providers and researchers.  This system
will allow researchers to scale their clustering problems without the need
for  specialized equipment or computing resources.  Cloud based clustering
will allow researchers to arbitrarily scale processing needs using the
virtually limitless commercial processing resources of large organizations.

\section{Background} Data clustering is a useful tool in pilot data and
knowledge engineering.	It is used in a range of problems from practical
data localization and search, to helping augment the understanding of complex
biological, social, and physical structures.  Although the problem itself has
been shown to be NP-Hard\cite{dasgupta08,Mahajan09}, many algorithms exist that
usually terminate in polynomial time with good results.  
\subsection{Big Data Clustering} In light of the ``Big Data'' revolution, many 
machine learning algorithms have been adapted to work with very large, often
distributed datasets.
Data storage costs have exponentially decreased, while raw clock speeds, 
commonly on a similar exponential trajectory, have stagnated\cite{cownie}. To 
maintain the expected Moore's law speedup, chip-makers have resorted to 
increasing the number of processing units per microprocessor. Overall this has 
resulted in a need for new parallel algorithms that support both massive 
datasets while also leveraging parallel processing architectures.

The goal of clustering remains the same, however memory and network constraints 
tend to dominate the running time requirements over algorithmic complexity 
alone. \emph{RPHash} is a clustering algorithm designed from the ground up to 
deal with these problems head on.  Instead of modifying an existing algorithm 
to perform better across networks and distributed systems, \emph{RPHash} trades
redundant computation for lower memory and network bandwidth requirements
through the use of random projections, and universally deterministic hashes.
\subsection{Random Projection Methods} Random projection is a method where
objects are projected to a lower dimensional subspace by a projection
matrix composed of Gaussian random variables.  Somewhat counter-intuitive
is that the random low dimensional subspace embedding is very near the
optimal subspace embedding preserving the original data with minimal
distortion\cite{bourgain1985lipschitz}.  The resurgence of the random
projection method of Johnson and Lindenstrauss was reinvigorated with the
work of Achlioptas on Database Friendly Projections\cite{Achlioptas01},
that provided good subspace embeddings requiring minimal computation cost.
\begin{Theorem}[JL - Lemma \cite{vempala}]
 $$
(1-\epsilon) \|u-u'\|^2 \leq \|f(u)-f(u')\|^2 \leq (1+\epsilon) \| u-u' \|^2
$$
\end{Theorem}
Johnson-Lindenstrauss lemma gives a tight bound for discretion of $n$ vectors
subjected to random projections in $\mathbb{R}^d$, $\Theta( {{log(n)}\over
  {\varepsilon^2 log(1/\varepsilon) }})$.
Vempala gives a relaxation of the JL-bound of $d \sim \Omega(log(n))$
\cite{vempala}, with a scaling factor of ${{1}\over{\sqrt{n}}}$ to preserve
approximate distances between projected vectors.

An additional benefit of Random Projection for mean clustering, is that
randomly projected asymmetric clusters tend to become more spherical in lower
dimensional subspace representations\cite{bingham}.  Mean and mediod clustering
algorithms, like \emph{RPHash}, are predisposed to sphereical clusters.

\subsection{Discrete Subspace Quantizers} Switching from the continuous spaces
of random projections, a discussion on discrete space partitioning lattices
is pursued.  For parallization of the clustering problem, the data space must
be partitioned as evenly as possible.  Furthermore, a universally generative
naming scheme must be established.  For known datasets, the perfect partition
of the data space is produced by the Voronoi partitioning\cite{Klein1988}.
In 2-dimensional space the Voronoi Diagram can be generated in $\Theta(n
log(n))$-time \cite{Fortune}, however higher dimensional algorithms
have less favorable run time complexities\cite{Gavrilova2003}, making
them inefficient for partitioning arbitrary dimensions and consequently,
\emph{RPHash}.	Instead, a lattice structure is considered, with rotation
and shifting transformations to compensate for gaps between spheres.
\begin{Definition}[Lattice in $\mathbb{R}^n$\cite{Pless}]
let $v_1, ...  , v_n$ be $n$ linear independent vectors where $v_i=v_{i,1}, 
v_{i,2}, ...  ,v_{i,n}$ 
The lattice $\Lambda$ with basis $\{v_1, ...  , v_n\}$ is the set of all 
integer combinations of $v_1, ...  , v_n$
the integer combinations of the basis vectors are the points of the lattice.
$$\Lambda = \{z_1v_1+z_2v_2+ ...  +z_nv_n | z_i\in \mathbb{Z}, 1 \leq i \leq 
n\}$$
\end{Definition}
Regular lattices formed from binary codes (such as the the E8 Lattice) have
natural enumerations which can be used as labels for the partitioned regions.
The mapping from a continuous space region to discrete binary sequence is a
hash function.	Furthermore, a hash functions with collision probability
dependent on a distance function is called a locality sensitive hash
function.
\begin{Definition}[Locality Sensitive Hash Function\cite{Datar}]
let $\mathbb{H}=\{h:S \rightarrow U\}$ is $(r_1,r_2,p_1,p_2)-$sensitive if for 
any $u,v\in S$
 \begin{enumerate}
   \item if $d(u,v) \leq r_1$ then $Pr_{\mathbb{H}}[h(u)=h(v)]\geq p_1$
   \item if $d(u,v) > r_2$ then $Pr_{\mathbb{H}}[h(u)=h(v)]\leq p_2$
 \end{enumerate}
\end{Definition}
%Although the combination of Random Projection and Space Partitioning is not 
%new \cite{Indyk} its use in parallel computation is. ball trees, motifs, 
%lattices signal theory, Shannon Limit
\subsection{Leech Lattice Decoder}
\begin{figure}%{r}{0.55\textwidth}
\centering
\includegraphics[scale=0.30]{doc/parallel.pdf}
\caption{\label{leech}Parallel Leech Decoder\cite{carraher}}
\end{figure}
The Leech Lattice is a unique 24 dimensional lattice with many exceptional
properties beyond those focused on in this proposal\cite{Curtis,SPLAG}.
Of particular interest to this proposal, is the Leech Lattice's packing
efficiency.  The Leech Lattice defines an optimal regular sphere packing of
24 dimensional space\cite{leech} and will serve nicely as a space quantizer
for \emph{RPHash}.  Furthermore, the Leech Lattice, being the cornerstone
of many intriguing mathematical concepts, has various relationships with
other sub-lattices, which offer some useful algorithmic decompositions.

Figure \ref{leech} gives the parallel decoding algorithm for the Leech
Lattice from \cite{carraher-1} based on the hexacode decoder of Amrani
and Be'ery's '96\cite{Amrani} publication on decoding the Leech Lattice.
The sequential worse-case complexity of this decoder requires 519 floating
point operations.  This decoder was chosen due to its interesting relationship
with the $(24,12,8)$ Binary Golay Code, and the decomposition of the Golay
Code resulting in very efficient decoding algorithms.  Although higher
dimensional lattices exist, with comparable packing efficiency, in general
decoding complexity scales exponentially with dimension\cite{Tarokh1,Agrell}.

\subsection{Issues with random projection and high dimensions} The
\textbf{Curse of Dimensionality} Consider the distance between any two
points $x$ and $y$ in $\mathbb{R}^d$.  under Euclidean distance: $dist(x,y)
=\sqrt{\sum^{d}_{i=1}{(x_i-y_i)^{2}}}$.  Now if the vector is composed of
values that have a uniformly distribution, according to Ullman \cite{Ullman}
the range of distances are all very near the average distance between points.
Given this, it becomes difficult to differentiate between near and far
points, or in other words, the distance function looses its usefulness for
high dimensions\cite{Beyer,Han2006}.  The usefulness of this principle
metric in clustering suggests that approximate and exact algorithms
will have an intersection at some number of dimensions.  Although this
intersection is not guaranteed to be at a point of favorable clustering
accuracy, Vempala suggests random projection actually may solve some of
these issues\cite{vempala,bingham}.

The \textbf{Occultation Problem} arises when two discrete clusters in
d-dimensional space project in to two non-discrete clusters in a lower
dimensional space.  Specifically, the probability a u-occultation for
d-dimensional space projected to 1-dimensional space given in \cite{Urruty2007}
is: $1-{2 \over \pi} arcos({{r_1+r_2}\over{d-c}}) $ where $r_1,r_2$ is the
radius of the respective clusters, and $d-c$ is the distance between them.
The method employed in Urruty\cite{Urruty2007} is to repeating the projection
until a non-occulting projection is found.  The rate of convergence for
finding a non-occulting projection is given as: $$\underset {d\rightarrow
\infty}{lim}1-\bigg({{2(r_1+r_2)}\over {\pi \|d-c\|}}\bigg)^d = 1$$ Which is
exponential in $d$.  Recognized in \cite{Urruty2007}, this bound is slightly
more favorable than the actual convergence rate, as the independence of
distinct projection events is not guaranteed.  However, for \emph{RPHash},
the required 24-dimensional projection to the Leech Lattice's native subspace,
is nearly orthogonal when the projection matrix is constructed from a set
of i.i.d.  Gaussian distributed random variables\cite{vempala}.


\subsection{Mapreduce, Hadoop, and Cloud}
 \emph{RPHash} will utilize state of the art frameworks, built on existing
 open source cloud technologies using commercial resources.
\textbf{Cloud} is a somewhat nebulous term as it is often used to describe
anything involving web services.  In regards to this proposal, cloud will
always refer to externally provided computing resources, and in particular
Amazon's implementation of Hadoop on their EC2 web service.  \textbf{Mapreduce}
is a framework for data parallel processing using two functional programming
mainstays, the map and reduce function, In addition to providing a standard
parallel algorithmic structure, mapreduce also performs many of the standard
resource management tasks required for parallel processing.

\textbf{Hadoop} is a freely distributed, open source implementation of the
mapreduce framework.  Currently in its second generation utilizing the new
Yarn resource manager, Hadoop is a fully developed Map Reduce framework
that is highly optimized for parallel data processing.  Hadoop will provide the
parallel communication and networking infrastructure while the map and reduce
abstract methods will provide the parallel design of \emph{RPHash}.



\section{Proposed Research} Clustering algorithms offer insight to a multitude
of data analysis problems.  Clustering algorithm are however, often limited
in their scalability and parallel speedup due to communication bottlenecks.
 A variety of methods have been  proposed to combat this issue with varying
degrees of success.  In general, these solutions tend to be parallel patches
and modification of existing clustering algorithms.  \emph{RPHash} is a
clustering algorithm designed uniquely for low interprocess communication
distributed architectures.  Random projection clustering has been previously
explored in \cite{Urruty2007,florescu09,fernrandom,avogadri09}, on single
processing architectures.

Despite theoretical results showing that k-means has an exponential worse
case complexity \cite{Vattani}, many real world problems tend to fair much
better under k-means and other similar algorithms.  For this reason, clustering
massive datasets, although suffering from unbounded complexity guarantees, is
still a very active area in computing research.  Due to approximate solution's
real world proclivity to useful results, randomized methods are commonly
used tools for overcoming complexity growth. The concept of random projection
clustering is not new, having been explored in a variety papers involving
high dimensional data clustering\cite{Proclus}.
In the Random Projection
Hash(\emph{RPHash}) algorithm, both approximate and randomized techniques
are employed to provide a scalable, approximate parallel system for massive
dataset clustering.  To combat the curse of dimensionality(COD) \emph{RPHash}
performs multi-probe, random projection of high dimensional vectors to the
unique subset boundaries of the Leech Lattice($\Lambda_{24}$)\cite{Andoni}.

\subsection{Objective 1: Develop the  Sequential \emph{RPHash} Algorithm}
Although \emph{RPHash} is a parallel algorithm, initially a sequential variant 
will be developed to show its overall efficacy on various datasets for 
comparison with common clustering
algorithms such as K-means and mean shift.  The primary goal of this objective
is to develop the sequential per compute node portion of the algorithm.
A light sequential wrapper for the parallel centroid aggregation will be
created in this step to demonstrate and test.

\subsubsection{sequential algorithm}

An outline of the steps in \emph{RPHash} is given below, however some of
the aspects of its function in regards to randomness and approximation are
highlighted here.  One way in which the \emph{RPHash} algorithm achieves
scalability is through the generative nature of its region assignment.
Clustering region assignments are performed by decoding vector points
into partitions of the Leech Lattice.  The Leech Lattice is a unique
lattice that provides optimal sphere packing density among 24 dimensional
regular lattices\cite{Cohn}.  
\begin{figure}%{l}{0.44\textwidth}
\centering
\includegraphics[scale=0.48]{doc/projplane.pdf}
\caption{Random projection on a plane\label{projplane}}
\end{figure}
Although optimal, due to the curse of
dimensionality, the overall density is somewhat sparse, requiring that the
algorithm apply shifts and rotations to the lattice to fully cover the
$\mathbb{R}^{24}$ subspace.  Furthermore, in general vectors will be greater 
than 24 dimensions.  The Johnson-Lindenstrauss (\emph{JL}) lemma to provide a
solution to
this problem (Figure \ref{projplane}).  \emph{JL} states that for an arbitrary
set of n
points in m dimensional space, a projection exists onto a d-dimensional 
subspace such that all points are linearly separable with $\epsilon$-distortion
 following 
 $d \propto {\Omega({{ log(n) }\over {\epsilon^2 log 1/\epsilon}  })}$.
 Although many options for projections exists, a simple and sufficient 
method for high dimensions is to form the projection matrix 
$r_{ij}\in\textbf{R}$ is $m\times d$ as follows:
\[
    r_{ij}= 
\begin{cases}
    +1, & \text{with probability } {1 \over 6}\\
     0, & \text{with probability } {2 \over 3}\\
    -1, & \text{with probability } {1 \over 6}\\
\end{cases}\text{\cite{Achlioptas01}}
\]
The approximation of a random projection is computationally
efficient for large datasets, and unlike a truly Gaussian projection matrix,
yields a semi-positive definite projection matrix, which will likely be
useful in proving the convergence of \emph{RPHash}.

In addition to Achlioptas efficient random projection method for
databases, a further reduction in the number of operation required
for random subspace projection called the Fast Johnson Lindenstrauss
Transform(FJLT)\cite{ailon2006,Dasgupta,ailon2013} is currently a very
active area of research.  FJLT and similar nearly optimal projection methods
utilize the local-global duality(Heisenberg Principle) of the Discrete
Fourier Transform to precondition the projection matrix resulting in a
nearly optimal number of steps to compute an $\epsilon$-distortion random
projection\cite{ailon2006,Dasgupta,ailon2013}.	A sub-linear bound on the
number of operations required for the dominant projection operation may
further improve \emph{RPHash}'s overall runtime complexity.


\begin{figure}%{r}{0.55\textwidth}
\centering
\includegraphics[scale=0.55]{doc/randproj.pdf}
\caption{\label{randproj}Multiple Projection Region Assignments and
Cardinalities}
\end{figure}

\textbf{Overview of the sequential algorithm:} The
basic intuition of \emph{RPHash} is to combine multi-probe random projection
with discrete space quantization.  Following this intuition, near-neighbor
vectors are often projected to the same partition of the space quantizer, 
which is regarded as a hash collision in LSH parlance. As an extension, 
multi-probe projection ensures that regions of high 
density in the original vector space are projected probabilistically more 
often to the same partitions that correspond to density modes
in the original data. In other words, partitions with high collision rates
are good candidates for cluster centroids.
To follow common parameterized, k-means methods, the
top k densest regions will be selected. Preliminary results of this process
can be found in Figure \ref{praccuracy}, in which k-means(green) and a 
simplified \emph{RPHash}(blue) algorithm were compared using 50:50 
precision-recall analysis for 
varying number of Gaussian distributed clusters and number of dimensions 
respectively. From the result, \emph{RPHash}'s accuracy appears to be a viable,
somewhat worse performing alternative to k-means clustering. 
\begin{figure}
        \centering
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{doc/PRvaryClusters}
                %\caption{Varying Clusters}
                \label{PRaccClu}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{doc/PRvaryDim}
                %\caption{Varying Dimensions}
                \label{PRaccDim}
        \end{subfigure}
	 \caption{Precision-Recall for K-Means(green) and \emph{RPHash}(blue) 
              varying Vectors and Clusters}\label{praccuracy}
\end{figure}

According to
\emph{JL} lemma, the sub-projections will conserve the pairwise distances
in the projected space for points with $\epsilon$-distortion in which
the size of the dataset is proportional to the logarithm of the number of
dimensions in the randomly projected subspace.	In addition to compressing
a dataset to a computationally more feasible subspace for performing space
quantization, random projection can also make \emph{eccentric} cluster more
spherical\cite{Dasgupta2000,vempala}.

Discrete space quantizers play a central role in the \emph{RPHash} algorithm.
The sequential implementation of the \emph{RPHash} algorithm will rely on the 
efficient Leech lattice decoder of Vardy, Sun, Be'ery, and Amrani
\cite{Vardy95,Sun,Be'ery,Amrani} used as a space quantizer.  The lattice
decoder implementation relies on the decomposition of the binary $(24,22,8)$
extended Golay Code into 4 partitions of the $(6,3,4)$ quaternary hexacode
and its relation to the Leech Lattice as a type B lattice construction.
This decomposition and relation to the Golay Code provides a real worse
case decoding complexity well below the asymptotically exponential bound
for trellis decoders as the dimension $d$ increases\cite{Tarokh1,Tarokh2}.
The Leech lattice is a unique lattice in 24 dimensions that is the densest
lattice packing of hyper-spheres in 24 dimensional space\cite{leech,SPLAG}.
The Leech lattice is a unique lattice in 24 dimensional space with many
exceptional properties, however of importance to \emph{RPHash}, is that it
is the densest regular lattice possible in 24 dimensions and nearly optimal
among theoretical non-regular packings\cite{Cohn}.  The 24 dimensional
subspace partitioned by the Leech Lattice is small enough to exhibit the
spherical clustering benefit of random projection.  Low distortion random
embeddings are also feasible for very large datasets ($n = \Omega(c^{24})$)
objects while avoiding the occultation phenomena\cite{Urruty2007}.
Furthermore, the decoding of the Leech lattice is a well studied subject,
with a constant worse case decoding complexity of 331 operations\cite{Vardy95}.

Space quantizers have hard margin boundaries and will only correctly
decode points that are within the error correcting radius of its
partitions.  This is an an issue found in approximate nearest neighbor
search \cite{panigrahy,Andoni}, and is overcome in a similar way as
Panigrahy\cite{panigrahy}, by performing multiple random projections
of a vector then applying the appropriate locality sensitive to
provide a set of hash IDs.  Using many random projections of a vector
allows the high dimensional vector to be represented as 'fuzzy' regions
probabilistically dependent on the higher dimensional counterpart.  From
Panigrahy\cite{panigrahy}, the requirement of ($\Theta(log(n))$) random
projection probes is given to achieve c-approximate hash collisions for the
bounded radius, r-near vectors.  Figure \ref{randproj} shows an example of
this process as a set of probabilistic collision regions.  Random projection
probing adds a $\Theta(log(n))$ -complexity coefficient to the clustering
algorithm but is essential for compute node independence as will be addressed
in the parallel extension of this algorithm.  The intuition of this step is to
find lattice hash IDs that on average generate more collisions, than others.
The the top $k$ cardinality set of lattice hash ID vector subsets represent
regions of high density.  The centroids are computed from the means of the
non-overlapping subsets of vectors for each high cardinality lattice hash ID.
The algorithm used above has a natural extension to the parallel algorithm
describe in the following section.

In Figure \ref{timecomplex}, initial analysis of time complexity for 
k-means(green)
and \emph{RPHash}(blue) on a single processing node, suggest an overall linear complexity
for varying number of data vectors and dimensions on Gaussian data. For a 
single processor, K-means surpasses \emph{RPHash}. The result is not unexpected
however, as \emph{RPHash} intentionally trades sequential processing complexity
for parallel communication independence. 
\begin{figure}
        \centering
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{doc/TimevaryPart}
                %\caption{Varying Vectors}
                \label{TimePart}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
                \includegraphics[width=\textwidth]{doc/TimevaryDim}
                %\caption{Varying Dim}
                \label{TimeDim}
        \end{subfigure}
	  \caption{Computation Time for K-Means(green) and \emph{RPHash}(blue) 
               varying Vectors and Dimensions}\label{timecomplex}
\end{figure}

\subsection{Objective 2:  Develop A distributed version of the proposed
\emph{RPHash} Algorithm} A key aspect of generative groups such as the
IDs of lattice centers in the Leech Lattice, is that their values are
universally computable.  For distributed data systems this aspect provides
complete compute node independence.  Furthermore, r-near neighbor hash
collisions for projected vectors do not need to share a particular basis.
Instead they need only be randomly projected a bounded number of times
according to results found in Panigrahy\cite{panigrahy}.  Using these tools,
the \emph{RPHash} algorithm is mostly naively parallel.  The overall parallel
portion of the algorithm is in fact no more complicated than a parallel sum
algorithm, or reduce operation on Hadoop.  
%Parallel prefix sum which has a work efficient parallel implementation of 
%Ladner requiring $\Theta(n)$-steps
%and $\Theta(log(c))$ communications for $c$ computing nodes\cite{Ladner}.

Algorithmic adaptations to from sequential \emph{RPHash} to a parallel 
sum problem are yet another common
tool for acheiving a work efficient scalable parallel algorithm.
Due to the work efficient parallel complexity of the reduce function, the
overall complexity of \emph{RPHash} is asymptotically no worse than its per
compute node sequential complexity.  This complexity, as previously stated
is dominated by the number of hash probes required to assure near-neighbor
collisions for random projections of the $n$ vectors($\Theta(nlog(n))$).  

Another common issue with parallel scalability,
is the data transfer bottleneck.  Although data transfer overhead cannot be
completely eliminate, it can be minimize significantly be decoupling the
data using generative groups relying on probable outcomes.  \emph{RPHash}
achieves this by adopting a two step approach to clustering with little
negative effect on accuracy.  The one step data transfer requirement for
directly applied parallel sum is on the order of the dimensionality of the
vectors and the expected number of large clusters which may be parameterized
as some $\epsilon k$.  Furthermore, the real communication requirement can be 
further minimize by only gathering the computed IDs, and using the fact that 
the smallest, top $k$ cluster's cardinality $\hat{c_{k}}$, must be, at most
greater than all other compute node's partial sums.  Otherwise, there must
exists a cluster $\hat{c}_{k+1}$, where $|\hat{c}_{k+1}| > |\hat{c}_{k}|$,
therefor $\hat{c}_{k}\nsubseteq C$ and should not be a candidate centroid.
The steps will be summarized here, as well as in standard pseudo-code
algorithmic form (\ref{Phase1} and \ref{Phase2} respectively).


%To achieve the previous goals, we alter the
%standard mean shift kernel gradient  descent algorithm to common density modes
%by separating it into a two step  course-grain and fine-grain shift.  By
%adjusting the granularity of the shift,  it is althought that we can control 
%the compute node inter-dependency.    Furthermore, one goal of this paper is 
%to be able to draw a correlation  between the $\epsilon$-error of our 
%algorithm to optimal clustering, and the  course-grain to fine-grain ratio 
%parameter.\\
%\begin{wrapfigure}{r}{.5\textwidth}
%\centering
%\[
%\overset{\rightarrow}{x}= {{\sum_{x_i\in N_(x)}
%{K'({x-x_i})\overset{\rightarrow}{x}}}\over {\sum_{x_i\in N_(x)} {K'({x-x_i)}}
%}}
%\]\caption*{Mean-Shift Iteration\cite{Fajie}}
%\end{wrapfigure}
%\textbf{Fine-Grain Iteration:}
\textbf{Phase 1 of Parallel \emph{RPHash}: Maximum Bucket Counting:}

As \emph{RPHash} is a distributed algorithm, the data vectors reside on
$D$ independent computing nodes, connected through some non-specific,
Hadoop\cite{hadoop} compatible network hardware.  The system architecture
will utilize the Yarn \emph{MR}v2 resource manager for task scheduling and
network communication load balancing. With much of the parallel task scheduling
 details handled by Yarn \emph{MR}v2, the following algorithm description will
focus on the per compute node task level processing.  

As the random projections can
be performed independently, the compute node interdependency is very low.
Due to the Leech lattice's set dimensionality, conversion between vectors of
$\mathbb{R}^d$ and $\mathbb{R}^{24}$  will be performed by a random projection
matrix of random variables following a nearly Gaussian distribution as in
Achlioptas\cite{Achlioptas01}.  Therefor the first parallel task is to 
distribute a random projection matrix to each compute node.  The communication 
load can be further minimized by noting that the random projection matrix,
although, stated as random, is actually the result of a pseudo-random 
computation.

A trade-off exists between the communication cost of distributing a large
$\mathbb{P}_{m\rightarrow d}$ and the redundant calculation of such data
per compute node.  Parallel processing tuning is an effective solution to
this problem, however in \emph{RPHash}, the generative method is chosen,
due to the constant scalability barrier that results from network bandwidth
saturation. In addition empirical results from Chowdhury\cite{chowdhury} 
suggesting
that the ratio of communication to computation increases with the number of
MR nodes.  

With the initial data distribution requirements taken care of,
the focus is on per node computation.  Each compute node processes
its set of vectors independently; first projecting the vector to a random 
subspace,
then decoding the projected vector to its nearest Leech lattice centroid ID.  
The lattice region
IDs are generative and thus identical across all compute nodes, an aspect 
discussed further in the parallel algorithm description.  In the
`bucket counting' phase, the number of collisions each lattice ID receives
is the only needed data.  Once all data vectors have been processed and
their collisions accumulated, the standard parallel sum reduce algorithm
implemented in MapReduce will be used to accumulate the lattice ID counts across all
compute nodes.	The set of lattice ID counts is sorted, and the $k$-largest
Leech lattice IDs are broadcast to all compute nodes for Phase 2 processing.

The bucket count aggregation step is susceptible to missing top k buckets
under non-random density mode distribution among computing nodes.  Initial
testing, Figure \ref{reduxcounts}, on power law distributed bucket cardinality, 
a common distribution for object-class memberships in natural processes
\cite{reed2002}, shows diminishing effects on the missed cluster rate for 
dataset sizes($10^5$(red),$10^{5.5}$(green),$10^6$(blue)) as the algorithm
scales to more processors. In general the miss rate seems to converge around
0.5 misses per node aggregation phase. The decreasing missed bucket rate corresponding to
the overall dataset size increase, can be attributed to the ``law of large 
numbers'', which results in each processing node being assigned a partition of
bucket counts that is representational of the overall dataset. Furthermore,
Karp et al\cite{Karp} suggest a simple solution to a similar problem of finding
frequent item sets in streams, that would provide a deterministic solution to this
problem.

\begin{figure}%{l}{0.44\textwidth}
\centering
\includegraphics[scale=0.22]{doc/varynodes.pdf}
\caption{Random projection on a plane\label{reduxcounts}}
\end{figure}

\begin{algorithm}
\caption{Phase1 \emph{RPHash} Clustering}\label{Phase1}
\begin{algorithmic}[1]
\REQUIRE $X=\{x_1,...,x_n\}$, $x_k \in \mathbb{R}^m$ - data vectors\\
 $D$ - set of available compute nodes\\
$\mathbb{H}$ - is a d dimensional LSH function\\
 $\widetilde{X} \subseteq X$ - vectors per compute node\\
 $\mathbb{P}_{m\rightarrow d}$ - Gaussian projection matrix
\STATE $C_s=\{\varnothing\}$ - set of bucket collision counts
\FORALL{$x_k \in \widetilde{X}$} 
\STATE $\tilde{x_k}\leftarrow \sqrt{{m}\over{d}}\mathbb{P}^{\intercal}x_k $
\STATE $t = \mathbb{H}(\tilde{x_k})$
\STATE $C_s[t]=C_s[t]+1$
\ENDFOR
\STATE sort($\{C_s,C_s.index\}$)
\State \textbf{return}$\{C_s,C_s$.index$\} [0 : k]$
%\caption*{Phase1 RP-Hash Clustering}
\end{algorithmic}
\end{algorithm}
%\end{wrapfigure}





\textbf{Phase 2 of Parallel \emph{RPHash}: } 
The two phased approach
for clustering is motivated by the work of Panigrahy, Andoni, and
Indyk's\cite{panigrahy,Andoni} on linear-space cr-approximate nearest neighbor
search(cr-ANN).  In their work they showed using time space tradeoff, that
moving the multi-random projection hashing phase could be moved to the search
portion of the algorithm with no effect on the overall asymptotic complexity
of the algorithm.  A similar tradeoff will be employ.  In the first phase of
\emph{RPHash},	the candidate centroid database is constructed of the highest
cardinality lattice hash buckets.  While actual searching of the database is
performed in the second phase.	\emph{RPHash} is effectively and inversion
of cr-ANN, in the second phase, all vectors of the database are searched,
against the small subset of high cardinality lattice hash ID sets.  The set
of lattice hash IDs corresponding to the $k*log(n)$ largest lattice hash ID
subsets are broadcast to all compute nodes through standard MapReduce broadcast
methods.  In the case of overlapping lattice hash sets, clusters having similar
centroids, either a parameterized overlap factor or some reasonable discrepancy
value based on the data's information entropy is applied to the parameter $k$.
Overlapping subsets can be merged during the gather phase with little effect
on the overall computational complexity.  An alternative to be experimentally
explored, will be to simply assume buckets rarely overlap for clusters.
The experimental results of which can be directly applied to the results
of Urruty and Florescu, as validation that a 'good' subspace projection,
although shown to grow exponentially as d increases\cite{florescu09}, is
still bounded by the probability bound on the number of multiple random
subspace projections, which is also exponential\cite{Urruty2007}.

The per compute node processing in Phase 2 requires $log(n)$ random
projections\cite{panigrahy} for each vector to achieve lattice hash collisions
with high probability for neighboring vectors.	While processing the projection
and hashes, In addition to the vector counts from phase 1, the centroid partial
sums must also be stored.  However, instead of storing the partial sums of
candidate centroids for all possible hash IDs, \emph{RPHash} only stores the
partial vector sums of the highest cardinality, broadcast $k$ - $k*log(n)$
cluster IDs from phase 1.  After processing the $log(n)$-projections of $n$
vectors in addition to accumulating the per compute node centroid partial
sums ,a parallel log reduce sum is used to accumulate all vector sums and
frequency counts.  The centroid vectors for each partial vector sum are then
scaled using the set of lattice ID collision totals.


%\begin{wrapfigure}{l}{0.55\textwidth}
%
\begin{algorithm}
\caption{Phase2 \emph{RPHash} Clustering}\label{Phase2}
\begin{algorithmic}[1]
\REQUIRE $X=\{x_1,...,x_n\}$, $x_k \in \mathbb{R}^m$ - data vectors\\
$D$ - set of available compute nodes\\
$\{C_s,C_s.$index$\}$ - set of $klogn$ cluster IDs and counts\\
$\mathbb{H}$ - is a $d$-dimensional LSH function\\
$\widetilde{X} \subseteq X$ - vectors per compute node\\
$p_{m\rightarrow d}\in \mathbb{P}$ - Gaussian projection matrices
\STATE $C=\{\varnothing\}$ - set of centroids
\FORALL{$x_k \in \widetilde{X}$} 
\FORALL{$p_{m\rightarrow d} \in \widetilde{\mathbb{P}}$} 
\STATE $\tilde{x_k}\leftarrow \sqrt{{m}\over{d}}p^{\intercal}x_k $
\STATE $t = \mathbb{H}(\tilde{x_k})$
\IF{$t \in C_s.$index}
\STATE $C[t]=C[t]+x_k$
\ENDIF
\ENDFOR
\ENDFOR
\State \textbf{return} ${C}$
%\caption*{Phase2 RP-Hash Clustering}
\end{algorithmic}
\end{algorithm}
%\end{wrapfigure}
\subsubsection{Problems with finding good random subspaces} 
For a fixed number of random projections the
probability of finding a good projections converges exponentially to zero as
the number of dimensions $d$ increases\cite{florescu09}.  In this proposal,
the probability of finding 'good' (or low distortion) versus unfavorable
(high distortion) subspace projections, is improved by both random probing
and the use of a high dimensional projection subspace following the results
of Urruty\cite{Urruty2007}.

\subsection{Objective 3: Deploy \emph{RPHash} to EC2 Cloud's Hadoop
Implementation} An important goal of \emph{RPHash} and big data mining for
the computing community in general is accessibility to data scientists.
\emph{RPHash} is not focused on exotic, difficult to access systems, and
instead is intended to be built for cloud processing platforms.

The first requirement of deploying the parallel algorithm on a cloud
based platform is to create a simulated environment on a local system.
Virtualization will be used to create this simulated environment, and
scripts(vagrant, Chef, Puppet) will be written to handle the automatic
deployment of virtual machines running the Hadoop \emph{MR}v2 framework.
Automatic deployment scripts will help with generating results for the speedup
tests in discussed in the experimental design section for both locally and
off-site EC2 Cloud, deployments.

After a local deployment system is available, an Amazon EC2 cloud account will
be created for further testing.  The amazon account will be managed by the
researchers using funds from this grant and applied to application server
time on Amazon's pay-per-use cloud.  Most EC2 testing will be performed
around scalability testing on synthetic data.  As \emph{RPHash} is a new
algorithm, the focus will be more on how it behaves as a distributed system,
and less on its qualitative clustering accuracy among the chosen data sets.
This requirement will avoid the expenses associated with Amazon's services,
while also not violating potential copyright and privacy issues that could
arise from uploading to the cloud.


\subsection{Objective 4: Data Security} Although certain privacies are provided
by commercial cloud service providers, security is reliant on the private
companies security practices.  For certain problems, specifically those
involving an patient's health care data, privacy safeguards must undergo
rigorous auditing requirements.  Such auditing is beyond the scope of many
private cloud companies business models, creating a barrier for the health
care community in utilizing cloud based resources.  Furthermore, recent United
States government initiatives pushing for the large scale availability of
data resources have made vast quantities of de-identified health information
available to the public.  These resources however have prompted advances in
attacks on de-identification of whole genome sequence data.  Such attacks
have been used to to associate, thought to be, anonymous medical records
with specific individuals\cite{deident}.  Similar de-anonymization attacks
\cite{deanon1,deanon2} along with a presidential commission (privacy and
progress in WGS) have prompted a need for better data security of medical
records data.  The \emph{RPHash} algorithm provides an intrinsic solution
to this problem in both the distribution of data among servers as well as
during the communication steps required by the algorithm.

\emph{RPHash} is a distributed algorithm designed to compute clusters on
databases that could potentially span over the public internet.
 As such, data it uses to
compute clusters can theoretically reside independently among different health
care facilities with no requirement for any single location data storage
architecture.  As a consequence of projecting the real data vectors to a
random subspace via a near, but not completely orthogonal matrix, destructive
data loss occurs providing a cryptographic ''trap-door' function.  The data
loss is an intrinsic part of the \emph{RPHash} clustering algorithm that has
little adverse effect on its model generation and subsequent recall accuracy.

The only point at which fully qualified vectors are transmitted between compute
nodes is during the Phase 2 gather stage.  This however is not of concern as
the data represents an aggregation of individual data.	By definition of a
centroid it is the average of only the $k$ largest populations of patients.
\emph{RPHash} is a clustering algorithm for very large datasets, where the
probability of identifying an individual from Phase 2 vector
 information is proportional to the size of the smallest identified cluster.

\section{Experimental Approach}

To assure \emph{RPHash}'s accuracy and performance, tests for similarity
to other algorithms, as well tests on different datasets will be performed.
The experirimental approach for testing \emph{RPHash} will address 3 major
areas of \emph{RPHash}'s utility.  
\begin{itemize}
 \item Algorithm Accuracy 
\item Differing Data Set Accuracy 
\item Scalability
 \item Parallel Speedup 
\item System Security

\end{itemize}


\subsection{Algorithm Accuracy} A classic analysis of any clustering algorithm
is its ability to correctly categorize unseen data.  There are various ways of
doing this such as Precision Recall, ROC, and FP/TF with varying applicability
to different domain constraints.  A primary resource for comparison will
be the industry standard learning algorithms found in the Apache Mahout
Machine learning for Hadoop Project.  The algorithms in Mahout will serve
as a comparison set to evaluate the classification accuracy of \emph{RPHash}
using the standard win/draw/lose contest metric\cite{Webb2000}.  In addition,
the Mahout algorithms will give a comparison measure for overall speed and
scaling complexity of \emph{RPHash}.

\subsection{Differing Data Set Accuracy} To accurately test \emph{RPHash},
a variety of real and synthetic datasets will be incorporated.	The synthetic
datasets will consist of varying size and dimension, Gaussian clusters, while
the real datasets will be acquired from various bioinformatics databases,
as well as semantic data from health care resources.  An emphasis on health
care related data will be used to establish the distributed database security
utility of the algorithm.

The synthetic dataset will consist of $k$ vectors from $\mathbb{R}^d$ chosen
from a uniform random distribution.  These vectors will form the basis for
the cluster centers.  For each cluster center an $n/k$ set of vectors will
be generated by an additive Gaussian noise permutation added to the center
vector.  Ultimately this will give $n$ vectors in $k$ Gaussian clusters.
From this half of the vectors will be reserved for training, and the other
half for testing accuracy.

The real dataset will consist of genomic data and image SIFT vector data.
These sets will give us a respective demonstration of \emph{RPHash}'s
performance on very high dimensional, and relatively high dimensional datasets.

\subsection{Scalability} Although \emph{RPHash} is expected to be comparably
to other clustering algorithms, the primary goal of the algorithm is not to
greater accuracy rather it is scalability on larger data set problems and
better computing resource utilization.	Standard scalability tests will focus
on varying dataset sizes on synthetic data.  Although real data would provide
more realistic conditions, it is sufficient, in regards to proving scalability
bounds, to use synthetic data.	Unlike iterative methods, the \emph{RPHash}
algorithm's complexity is not adaptive to the data.  Communication, and
iterations are explicitly set by the size and dimensionality of the data, not
the inherent clusters.	The theoretical asymptotic complexity of \emph{RPHash}
is bound by the required random projection probes to achieve near neighbor
bucket collisions $\Theta({n log(n)})$.

\subsection{Parallel Speedup} Another one of the focused aims of \emph{RPHash}h
is the computational gains resulting from the addition of compute nodes.
For many algorithms the achievable speedup is restricted by Amdahl's law for
scalability.  In which the ratio of sequential to parallelizable code reduces
the available speed up even for large ratios.  For this reason, \emph{RPHash}
is an attempt eliminate the sequential bottleneck.  Experimentally, the
scalability will be demonstrated using a varying number of nodes in the
EC2 cloud.  Although scaling will be performed across a large set of nodes,
extrapolation will also be used to characterize the experimental scalability
with the theoretical scalability.  Furthermore, these results will validate
the claims for \emph{RPHash}'s overall theoretical parallel complexity and
scalability while accounting for communication overhead.

An as of yet, uncertain aspect of the \emph{RPHash} algorithm, is to avoid
overlapping clusters and not disregard cluster that could potentially garner
enough support to become clusters when accumulated over the many nodes in the
parallel system.  The potential options change the communication overhead,
and therefor tests of the various implementations will be compared to find
the best overall parallel speed up.  In the untested collision variant,
the communication overhead is the complexity of the gather operation
over the nodes, number of clusters and dimension of the data vectors
($\Theta(kd{log(N))})$).

\subsection{Security Tests} As the main goal is to show \emph{RPHash}'s
resistance to de-anonymization attacks by use of destructive projection
functions, the security demonstration will be predominantly empirical.
Proving the security of a system is often not possible, and stating such
would be misleading.  Instead, showing that no single, fully qualified record
can be associated with the data sent between Hadoop nodes will suffice to
show \emph{RPHash}'s added data security feature.  Fully auditing security
is a difficult task, and is somewhat beyond the scope of this proposal,
Furthermore, the final Hadoop implementation may expose side channel attacks,
which would be beyond the scope of the \emph{RPHash} algorithm.

Demonstrating the level of obfuscation of the data communicated between
nodes running \emph{RPHash} will be used to suggest its level of security.
A common method for reconstructing a vector from its projection would be to
invert the projection matrix, and apply it to the projected vector.  This of
course is not feasible for singular matrices and because the projections are
specifically subspace projections, they will not be square.  However there
is some data leakage in the form of communicating the Lattice hash IDs for
each vector.  The hash ID represents a 24 dimensional vector, and having
multiple of which could possibly allow for a least squares pseudo inverse of
the projection matrix that could be used to recover information.  Comparing
known, correlated, fully qualified vector $u$ and the randomly projected
counterpart vector $v$ and its projected inverse, using the least squares
pseudo-inverse of the projection matrix $\hat{R}^{-1}$ as the mapping back
to the original space, will give a qualitative measure of data obfuscation.
$ v = \sqrt{{{n}\over{k}}}R^Tu $, $ v' = \sqrt{{{k}\over{n}}}v^T \hat{R}^{-1}
$, $ similarity = ||v,v'||_{2} $.

For orthogonal projection matrices the least squares solution serves as a
suitable inverse for the projection function.  However, for non-orthogonal
projection matrices the least-squares solution is over-determined, and the
basis vectors will overlap, causing unrecoverable data loss.  The goal in
this test of \emph{RPHash} is to show that the data loss is enough to make
it very difficult to re-associate vectors with any projected data shared
among Hadoop Nodes while also maintaining enough internal correlation to
allow for accurate clustering.

\section{Related Work} Clustering is a fundamental machine learning algorithm
and therefor has been extensively applied to a variety of data analysis
problems on a wide variety of platforms.  Vector distance computation and
near neighbor search, requirements innate to data clustering, are naively
parallelizable.  However scalability of clustering in general despite
parallelization of these aspects is none-the-less dominated by communication
bottlenecks.  Due to the inherent scalability issues of many sequentially
designed algorithms, parallel algorithms have also been developed to mitigate
these issues.  \subsection{Density Scan Database Clustering Methods}
The first set of parallel clustering algorithms began with density based
scanning methods.  These methods tend to work well on spatially separated
datasets with relatively low dimension.  A very common clustering problem for
these types of algorithms would be on geo-spatial data, such as large maps and
image segmentation.  The algorithms DBScan\cite{dbscan}, Clique\cite{clique},
and CLARANS\cite{Clarans}, respectively, represent a successful progression
of the density scanning techniques.  Although density scan algorithms are an
example of parallel designed algorithms like \emph{RPHash}, they often show
weaknesses in accuracy when scaling the number of dimensions.  A proposed
solution mentioned below to this problem is PROCLUS\cite{Proclus}.

\subsection{Random Projection Techniques} Another important feature of
\emph{RPHash} is the use of random projection clustering.  A proof
of the convergence of projected k-means clustering is given in
Boutsidis\cite{Boutsidis}.  Proclus\cite{Proclus} was an even earlier
application that explored random projection for clustering.  Proclus used
an assumption similar to \emph{RPHash} regarding high dimensional data's
sparseness.  Feature subset selection offers a way of compacting the
problem as well as removing artifacts.	Many subset selection algorithms
have been explored\cite{subset1,subset2,Yang,Kaski98}, they often require
difficult to parallelize iterative gradient descent\cite{Amdahl} or tree
traversal\cite{Freeman}.  Random projection is performed on the data to
compress it to spherical clusters\cite{Dasgupta2000} in a dense subspace.
An iterative method for k-medoid search, similar to CLARANS\cite{Clarans} is
then used to identify clusters.  Although the proposed method is successful
in accelerating larger dimensionality problems ($~d=20$) it does not have
the overall proposed scalability of \emph{RPHash}.  This is due to Proclus
being based on an ultimately iterative algorithm, containing inevitably
sequential code, and therefor lacking indefinite scalability\cite{Amdahl}.

In addition to PROCLUS, various other methods and analysis have been
performed on clustering with random projections that provide some bounds on the
convergence and extensibility of random projection clustering.	Florescu gives
bounds on the scaling and convergence of projected clustering\cite{florescu09}.
Their results closely follow the logic of Urruty\cite{Urruty2007}, and find
that the number of trial projections, or or more favorably, the number of
orthogonal, projected dimensions, effects the convergence rate exponentially.
Where the base is related to the arctangent of the angle between two distinct
clusters.  They also assert the probability of random projection offering a
good partitioning plane decreases exponentially as the number of dimensions
increases.  This suggests \emph{RPHash} may have to find a way to overcome
this issue, or show empirically that the asymptotic behavior of increasing
projected dimensions, dominates the increase in $d$.

Other clustering by random projection algorithms have been explored that are
very similar to the proposed method, but for projections on the line.  These so
called cluster ensemble approaches \cite{fernrandom,alweighted06,avogadri09}
use histograms to identify cluster regions of high density much like
\emph{RPHash}.	Although, as suggested in Florescu and Urruty, the single
dimension approach may be plagued by issues of occultation, and exponential
convergence as $d$ increases.



\section{Proposed Research Schedule} The proposed project duration is 2
years.	Project development will be performed at the University of Cincinnati
(UC), following the schedule below in 6 month increments.  
\begin{itemize}
\item\textbf{First}: Initial development of the sequential \emph{RPHash} will
be redesigned for efficiency, readability, and adaptability to parallel and
cloud based resources.	Sequential algorithm development will be transfered
from its current structure to a Map and Reduce architecture used in MRv2.
The algorithmic accuracy and theoretical scalability will be performed during
this period on synthetic and real world data.

\item\textbf{Second}: Parallel implementations of variants of the \emph{RPHash}
algorithm will be developed at the University of Cincinnati.  Parallel system
architecture and setup will be experimented with during this period, and
an optimal set of variants will be chosen to continue with the full scale
scalability and real world data testing on cloud resources.

\item\textbf{Third}: Cloud resources will be acquired, and the parallel
\emph{RPHash} algorithm will be deployed for initial testing on synthetic data.
Scalability testing will be performed on synthetic data prior to real world
biological data analysis.  Due to the lack of data security features in
the set of comparison algorithms, comparison data will only be performed on
synthetic data during this step.

\item\textbf{Forth}: Due to privacy concerns regarding biological data on
cloud resources, the secure data handling of \emph{RPHash} will be assessed
prior to any cloud processing.	Following favorable assessment of data
security, real world biological data will be processed using the \emph{RPHash}
algorithm on commercially available cloud resources.  \end{itemize}
\newpage
\pagenumbering{arabic}

\bibliography{refs}
\bibliographystyle{ieeetr} \markright{ }
\end{document}
